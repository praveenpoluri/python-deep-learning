{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prane\\anaconda2\\envs\\env_full\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\prane\\anaconda2\\envs\\env_full\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 48)\n",
      "(66292, 48)\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 48, 100)           200000    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 48, 64)            32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 24, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12, 100)           6500      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 6005      \n",
      "=================================================================\n",
      "Total params: 265,113\n",
      "Trainable params: 265,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prane\\Anaconda2\\envs\\env_full\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "156060/156060 [==============================] - 81s 519us/step - loss: 1.2704 - accuracy: 0.5066\n",
      "Epoch 2/4\n",
      "156060/156060 [==============================] - 81s 518us/step - loss: 1.1951 - accuracy: 0.5270\n",
      "Epoch 3/4\n",
      "156060/156060 [==============================] - 81s 519us/step - loss: 1.1200 - accuracy: 0.5648\n",
      "Epoch 4/4\n",
      "156060/156060 [==============================] - 82s 522us/step - loss: 1.0738 - accuracy: 0.5840\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "fpath=\"C:/Users/prane/Documents/Python and Deep Learning/LAB2/sentiment-analysis-on-movie-reviews\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_train = pd.read_csv(fpath+'/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv(fpath+'/test.tsv', sep='\\t')\n",
    "##Descriptive analyse\n",
    "#this should help you to decide whether to use STOP WORDS or not.\n",
    "#This part of code is just great analytical tool\n",
    "\n",
    "\n",
    "##Data preprocessing\n",
    "#we make text lower case and leave only letters from a-z and digits\n",
    "df_train['Phrase'] = df_train['Phrase'].str.lower()\n",
    "df_train['Phrase'] = df_train['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "df_test['Phrase'] = df_test['Phrase'].str.lower()\n",
    "df_test['Phrase'] = df_test['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "X_train = df_train.Phrase\n",
    "y_train = df_train.Sentiment\n",
    "max_fatures = 2000\n",
    "tokenize = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenize.fit_on_texts(X_train.values)\n",
    "X_test = df_test.Phrase\n",
    "X_train = tokenize.texts_to_sequences(X_train)\n",
    "X_test = tokenize.texts_to_sequences(X_test)\n",
    "max_lenght = max([len(s.split()) for s in df_train['Phrase']])\n",
    "X_train = pad_sequences(X_train, max_lenght)\n",
    "X_test = pad_sequences(X_test, max_lenght)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "##Model building\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_fatures, output_dim=100,input_length=48))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='causal'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='causal'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=4, verbose=True,  batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
