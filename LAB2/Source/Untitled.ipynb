{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        (os.path.join(dirname, filename))\n",
    "# Any results you write to the current directory are saved as output.\n",
    "labels = os.listdir('natural-images/natural_images/')\n",
    "print(labels)\n",
    "from IPython.display import Image, display\n",
    "num = []\n",
    "for label in labels:\n",
    "    path = 'natural-images/natural_images/{0}/'.format(label)\n",
    "    folder_data = os.listdir(path)\n",
    "    k = 0\n",
    "    print('\\n', label.upper())\n",
    "    for image_path in folder_data:\n",
    "        if k < 5:\n",
    "            display(Image(path+image_path))\n",
    "        k = k+1\n",
    "    num.append(k)\n",
    "    print('there are ', k,' images in ', label, 'class')\n",
    "\n",
    "\n",
    "x_data =[]\n",
    "y_data = []\n",
    "import cv2\n",
    "for label in labels:\n",
    "    path = 'natural-images/natural_images/{0}/'.format(label)\n",
    "    folder_data = os.listdir(path)\n",
    "    for image_path in folder_data:\n",
    "        image = cv2.imread(path+image_path)\n",
    "        image_resized = cv2.resize(image, (32,32))\n",
    "        x_data.append(np.array(image_resized))\n",
    "        y_data.append(label)\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "print('the shape of X is: ', x_data.shape, 'and that of Y is: ', y_data.shape)\n",
    "#stadardizing the input data\n",
    "x_data = x_data.astype('float32')/255\n",
    "\n",
    "#converting the y_data into categorical:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y_encoded = LabelEncoder().fit_transform(y_data)\n",
    "\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "#lets shuffle all the data we have:\n",
    "r = np.arange(x_data.shape[0])\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(r)\n",
    "X = x_data[r]\n",
    "Y = y_categorical[r]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n",
    "\n",
    "#structuring the CNN model\n",
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(rate=0.25))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(8, activation='softmax'))\n",
    "\n",
    "#let's compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "#fitting the model\n",
    "history = model.fit(X_train, Y_train, epochs=25, validation_split=0.2)\n",
    "\n",
    "#Display of the accuracy and the loss values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Loss/accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "Y_pred = model.predict_classes(X_test)\n",
    "#converting over Y test to actual labels.\n",
    "Y_test = np.argmax(Y_test, axis = 1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('the accuracy obtained on the test set is:', accuracy_score(Y_pred,Y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (1.25.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (1.16.4)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\prave\\anaconda3\\envs\\env_full\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 48)\n",
      "(66292, 48)\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 48, 100)           200000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 48, 64)            32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 24, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12, 100)           6500      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12, 100)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 6005      \n",
      "=================================================================\n",
      "Total params: 265,113\n",
      "Trainable params: 265,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prave\\Anaconda3\\envs\\env_full\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "156060/156060 [==============================] - 52s 334us/step - loss: 1.2581 - accuracy: 0.5098\n",
      "Epoch 2/4\n",
      "156060/156060 [==============================] - 52s 333us/step - loss: 1.1321 - accuracy: 0.5585\n",
      "Epoch 3/4\n",
      "156060/156060 [==============================] - 62s 395us/step - loss: 1.0620 - accuracy: 0.5851\n",
      "Epoch 4/4\n",
      "156060/156060 [==============================] - 61s 392us/step - loss: 1.0091 - accuracy: 0.6052\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_train = pd.read_csv('C:/Users/prave/OneDrive/Desktop/python/Lab2/sentiment-analysis-on-movie-reviews/train.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('C:/Users/prave/OneDrive/Desktop/python/Lab2/sentiment-analysis-on-movie-reviews/test.tsv', sep='\\t')\n",
    "##Descriptive analyse\n",
    "#this should help you to decide whether to use STOP WORDS or not.\n",
    "#This part of code is just great analytical tool\n",
    "\n",
    "\n",
    "##Data preprocessing\n",
    "#we make text lower case and leave only letters from a-z and digits\n",
    "df_train['Phrase'] = df_train['Phrase'].str.lower()\n",
    "df_train['Phrase'] = df_train['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "df_test['Phrase'] = df_test['Phrase'].str.lower()\n",
    "df_test['Phrase'] = df_test['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "X_train = df_train.Phrase\n",
    "y_train = df_train.Sentiment\n",
    "max_fatures = 2000\n",
    "tokenize = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenize.fit_on_texts(X_train.values)\n",
    "X_test = df_test.Phrase\n",
    "X_train = tokenize.texts_to_sequences(X_train)\n",
    "X_test = tokenize.texts_to_sequences(X_test)\n",
    "max_lenght = max([len(s.split()) for s in df_train['Phrase']])\n",
    "X_train = pad_sequences(X_train, max_lenght)\n",
    "X_test = pad_sequences(X_test, max_lenght)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "##Model building\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_fatures, output_dim=100,input_length=48))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='causal'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='causal'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=4, verbose=True,  batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
