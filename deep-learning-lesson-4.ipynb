{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveenpoluri/python-deep-learning/blob/master/deep-learning-lesson-4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xitplqMNk_Hc",
        "outputId": "ed4f60d2-878d-4056-c438-352dac39a112",
        "colab": {
          "height": 420
        }
      },
      "source": [
        "#@title Introducing Colaboratory { display-mode: \"form\" }\n",
        "#@markdown This 3-minute video gives an overview of the key features of Colaboratory:\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('inN8seMm7UI', width=600, height=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"400\"\n",
              "            src=\"https://www.youtube.com/embed/inN8seMm7UI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f956e9dda50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "The document you are reading is a  [Jupyter notebook](https://jupyter.org/), hosted in Colaboratory. It is not a static page, but an interactive environment that lets you write and execute code in Python and other languages.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "5626194c-e802-4293-942d-2908885c3c1f",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\".\n",
        "\n",
        "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gE-Ez1qtyIA",
        "outputId": "8d2e4259-4682-4e19-b683-7b9087f28820",
        "colab": {
          "height": 35
        }
      },
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lSrWNr3MuFUS"
      },
      "source": [
        "For more information about working with Colaboratory notebooks, see [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Rh3-Vt9Nev9"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "Learn how to make the most of Python, Jupyter, Colaboratory, and related tools with these resources:\n",
        "\n",
        "### Working with Notebooks in Colaboratory\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "- <img src=\"/img/new.png\" height=\"20px\" align=\"left\" hspace=\"4px\" alt=\"New\"></img>\n",
        " [TensorFlow 2 in Colab](/notebooks/tensorflow_version.ipynb)\n",
        "\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
        "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
        "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
        "\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "## Machine Learning Examples: Seedbank\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out the [Seedbank](https://research.google.com/seedbank/) project.\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Neural Style Transfer](https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras): Use deep learning to transfer style between images.\n",
        "- [EZ NSynth](https://research.google.com/seedbank/seed/ez_nsynth): Synthesize audio with WaveNet auto-encoders.\n",
        "- [Fashion MNIST with Keras and TPUs](https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus): Classify fashion-related images with deep learning.\n",
        "- [DeepDream](https://research.google.com/seedbank/seed/deepdream): Produce DeepDream images from your own photos.\n",
        "- [Convolutional VAE](https://research.google.com/seedbank/seed/convolutional_vae): Create a generative model of handwritten digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhkYfuDq7wVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "a09f1514-e996-4f00-9946-b14397637944"
      },
      "source": [
        "# Simple CNN model for CIFAR-10\n",
        "import numpy\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend.set_image_dim_ordering('th')\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "# Compile model\n",
        "epochs = 10\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               4194816   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 4,210,090\n",
            "Trainable params: 4,210,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 10s 202us/step - loss: 1.6799 - acc: 0.3940 - val_loss: 1.3431 - val_acc: 0.5160\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 10s 195us/step - loss: 1.3177 - acc: 0.5230 - val_loss: 1.1886 - val_acc: 0.5799\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 10s 194us/step - loss: 1.1655 - acc: 0.5840 - val_loss: 1.1044 - val_acc: 0.6112\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 10s 192us/step - loss: 1.0704 - acc: 0.6183 - val_loss: 1.0670 - val_acc: 0.6178\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 10s 195us/step - loss: 0.9931 - acc: 0.6449 - val_loss: 1.0009 - val_acc: 0.6432\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 10s 194us/step - loss: 0.9318 - acc: 0.6702 - val_loss: 0.9767 - val_acc: 0.6521\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 9s 190us/step - loss: 0.8852 - acc: 0.6865 - val_loss: 0.9599 - val_acc: 0.6604\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 10s 191us/step - loss: 0.8323 - acc: 0.7050 - val_loss: 0.9471 - val_acc: 0.6689\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 10s 193us/step - loss: 0.7958 - acc: 0.7181 - val_loss: 0.9369 - val_acc: 0.6708\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 9s 189us/step - loss: 0.7587 - acc: 0.7320 - val_loss: 0.9291 - val_acc: 0.6733\n",
            "Accuracy: 67.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ab495f07-5ad2-4b6e-dd3e-5aaac7a41292",
        "id": "uIsaLmkMPgkk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorboardcolab import *\n",
        "tbc=TensorBoardColab()\n",
        "\n",
        "\n",
        "# Simple CNN model for CIFAR-10\n",
        "import numpy\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend.set_image_dim_ordering('th')\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# flattening the matrix into vector form\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "epochs = 50\n",
        "lrate = 0.01\n",
        "decay = lrate/epochs\n",
        "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\",histogram_freq=0, write_graph=True, write_images=True)\n",
        "model.fit(X_train[0:2000], y_train[0:2000],validation_data=(X_test, y_test), nb_epoch=epochs, batch_size=32,callbacks=[TensorBoardColabCallback(tbc)])\n",
        "# Fit the mode\n",
        "#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "print(\"loss: %.2f%%\" % (scores[0]*100))\n",
        "model.save('./model' + '.h5')\n",
        "\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "print(\"loss: %.2f%%\" % (scores[0]*100))\n",
        "model.save('./model' + '.h5')\n",
        "import pandas as pd\n",
        "prediction = pd.DataFrame()\n",
        "imageid = []\n",
        "for i in range(len(X_test[0:4])):\n",
        "    i = i + 1\n",
        "    imageid.append(i)\n",
        "prediction[\"ImageId\"] = imageid\n",
        "prediction[\"Label\"] = model.predict_classes(X_test[0:4], verbose=0)\n",
        "print(prediction.head())\n",
        "\n",
        "import numpy as np\n",
        "a  = np.array(y_test[0:4])\n",
        "print('Actual labels for first four images: {0}'.format(np.argmax(a, axis=1)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://5b65c56b.ngrok.io\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_47 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 32, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 64, 16, 16)        18496     \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 64, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 64, 16, 16)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 64, 8, 8)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 128, 8, 8)         73856     \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 128, 8, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 128, 8, 8)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 128, 4, 4)         0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 2,915,114\n",
            "Trainable params: 2,915,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "2000/2000 [==============================] - 2s 1ms/step - loss: 2.3016 - acc: 0.1050 - val_loss: 2.2916 - val_acc: 0.1070\n",
            "Epoch 2/50\n",
            "2000/2000 [==============================] - 1s 655us/step - loss: 2.2420 - acc: 0.1530 - val_loss: 2.2031 - val_acc: 0.1937\n",
            "Epoch 3/50\n",
            "2000/2000 [==============================] - 1s 655us/step - loss: 2.2423 - acc: 0.1620 - val_loss: 2.3123 - val_acc: 0.1000\n",
            "Epoch 4/50\n",
            "2000/2000 [==============================] - 1s 651us/step - loss: 2.3059 - acc: 0.1065 - val_loss: 2.3014 - val_acc: 0.1125\n",
            "Epoch 5/50\n",
            "2000/2000 [==============================] - 1s 676us/step - loss: 2.1806 - acc: 0.1935 - val_loss: 2.1701 - val_acc: 0.2116\n",
            "Epoch 6/50\n",
            "2000/2000 [==============================] - 1s 650us/step - loss: 2.1014 - acc: 0.2305 - val_loss: 2.1125 - val_acc: 0.2174\n",
            "Epoch 7/50\n",
            "2000/2000 [==============================] - 1s 656us/step - loss: 2.0273 - acc: 0.2500 - val_loss: 2.0131 - val_acc: 0.2633\n",
            "Epoch 8/50\n",
            "2000/2000 [==============================] - 1s 667us/step - loss: 2.0114 - acc: 0.2530 - val_loss: 1.9906 - val_acc: 0.2642\n",
            "Epoch 9/50\n",
            "2000/2000 [==============================] - 1s 674us/step - loss: 1.9386 - acc: 0.2870 - val_loss: 2.0297 - val_acc: 0.2508\n",
            "Epoch 10/50\n",
            "2000/2000 [==============================] - 1s 657us/step - loss: 1.9139 - acc: 0.2885 - val_loss: 1.9677 - val_acc: 0.2772\n",
            "Epoch 11/50\n",
            "2000/2000 [==============================] - 1s 662us/step - loss: 1.8942 - acc: 0.3095 - val_loss: 1.8705 - val_acc: 0.3146\n",
            "Epoch 12/50\n",
            "2000/2000 [==============================] - 1s 668us/step - loss: 1.8874 - acc: 0.3130 - val_loss: 1.9101 - val_acc: 0.2991\n",
            "Epoch 13/50\n",
            "2000/2000 [==============================] - 1s 672us/step - loss: 1.8031 - acc: 0.3340 - val_loss: 1.8596 - val_acc: 0.3084\n",
            "Epoch 14/50\n",
            "2000/2000 [==============================] - 1s 674us/step - loss: 1.7728 - acc: 0.3440 - val_loss: 1.8199 - val_acc: 0.3366\n",
            "Epoch 15/50\n",
            "2000/2000 [==============================] - 1s 663us/step - loss: 1.7241 - acc: 0.3605 - val_loss: 1.8468 - val_acc: 0.3346\n",
            "Epoch 16/50\n",
            "2000/2000 [==============================] - 1s 665us/step - loss: 1.7012 - acc: 0.3790 - val_loss: 1.7856 - val_acc: 0.3489\n",
            "Epoch 17/50\n",
            "2000/2000 [==============================] - 1s 663us/step - loss: 1.6660 - acc: 0.3945 - val_loss: 1.8035 - val_acc: 0.3562\n",
            "Epoch 18/50\n",
            "2000/2000 [==============================] - 1s 659us/step - loss: 1.5826 - acc: 0.4075 - val_loss: 1.7036 - val_acc: 0.3867\n",
            "Epoch 19/50\n",
            "2000/2000 [==============================] - 1s 676us/step - loss: 1.5207 - acc: 0.4505 - val_loss: 1.7868 - val_acc: 0.3609\n",
            "Epoch 20/50\n",
            "2000/2000 [==============================] - 1s 656us/step - loss: 1.4772 - acc: 0.4565 - val_loss: 1.6986 - val_acc: 0.3850\n",
            "Epoch 21/50\n",
            "2000/2000 [==============================] - 1s 652us/step - loss: 1.4791 - acc: 0.4665 - val_loss: 1.6762 - val_acc: 0.4000\n",
            "Epoch 22/50\n",
            "2000/2000 [==============================] - 1s 655us/step - loss: 1.3722 - acc: 0.5215 - val_loss: 1.6050 - val_acc: 0.4245\n",
            "Epoch 23/50\n",
            "2000/2000 [==============================] - 1s 669us/step - loss: 1.3023 - acc: 0.5295 - val_loss: 1.6434 - val_acc: 0.4218\n",
            "Epoch 24/50\n",
            "2000/2000 [==============================] - 1s 671us/step - loss: 1.2646 - acc: 0.5460 - val_loss: 1.6838 - val_acc: 0.4010\n",
            "Epoch 25/50\n",
            "2000/2000 [==============================] - 1s 653us/step - loss: 1.2256 - acc: 0.5620 - val_loss: 1.6630 - val_acc: 0.4072\n",
            "Epoch 26/50\n",
            "2000/2000 [==============================] - 1s 668us/step - loss: 1.1468 - acc: 0.5815 - val_loss: 1.6884 - val_acc: 0.4046\n",
            "Epoch 27/50\n",
            "2000/2000 [==============================] - 1s 671us/step - loss: 1.0671 - acc: 0.6140 - val_loss: 1.7226 - val_acc: 0.4197\n",
            "Epoch 28/50\n",
            "2000/2000 [==============================] - 1s 669us/step - loss: 0.9837 - acc: 0.6490 - val_loss: 1.7209 - val_acc: 0.4291\n",
            "Epoch 29/50\n",
            "2000/2000 [==============================] - 1s 653us/step - loss: 0.8970 - acc: 0.6720 - val_loss: 1.7080 - val_acc: 0.4193\n",
            "Epoch 30/50\n",
            "2000/2000 [==============================] - 1s 660us/step - loss: 0.8918 - acc: 0.6675 - val_loss: 1.8459 - val_acc: 0.4147\n",
            "Epoch 31/50\n",
            "2000/2000 [==============================] - 1s 686us/step - loss: 0.7958 - acc: 0.7175 - val_loss: 1.8081 - val_acc: 0.4257\n",
            "Epoch 32/50\n",
            "2000/2000 [==============================] - 1s 643us/step - loss: 0.7307 - acc: 0.7340 - val_loss: 1.8295 - val_acc: 0.4261\n",
            "Epoch 33/50\n",
            "2000/2000 [==============================] - 1s 677us/step - loss: 0.6692 - acc: 0.7670 - val_loss: 2.0467 - val_acc: 0.4137\n",
            "Epoch 34/50\n",
            "2000/2000 [==============================] - 1s 654us/step - loss: 0.6325 - acc: 0.7750 - val_loss: 1.8798 - val_acc: 0.4295\n",
            "Epoch 35/50\n",
            "2000/2000 [==============================] - 1s 670us/step - loss: 0.5735 - acc: 0.7995 - val_loss: 2.0272 - val_acc: 0.4390\n",
            "Epoch 36/50\n",
            "2000/2000 [==============================] - 1s 685us/step - loss: 0.6061 - acc: 0.7840 - val_loss: 2.1256 - val_acc: 0.4350\n",
            "Epoch 37/50\n",
            "2000/2000 [==============================] - 1s 662us/step - loss: 0.4809 - acc: 0.8300 - val_loss: 2.1208 - val_acc: 0.4391\n",
            "Epoch 38/50\n",
            "2000/2000 [==============================] - 1s 663us/step - loss: 0.5001 - acc: 0.8300 - val_loss: 2.0857 - val_acc: 0.4123\n",
            "Epoch 39/50\n",
            "2000/2000 [==============================] - 1s 677us/step - loss: 0.3909 - acc: 0.8630 - val_loss: 2.4078 - val_acc: 0.4256\n",
            "Epoch 40/50\n",
            "2000/2000 [==============================] - 1s 655us/step - loss: 0.4089 - acc: 0.8470 - val_loss: 2.3171 - val_acc: 0.4075\n",
            "Epoch 41/50\n",
            "2000/2000 [==============================] - 1s 652us/step - loss: 0.3865 - acc: 0.8635 - val_loss: 2.3446 - val_acc: 0.4279\n",
            "Epoch 42/50\n",
            "2000/2000 [==============================] - 1s 674us/step - loss: 0.2974 - acc: 0.8910 - val_loss: 2.4117 - val_acc: 0.4280\n",
            "Epoch 43/50\n",
            "2000/2000 [==============================] - 1s 678us/step - loss: 0.2771 - acc: 0.9095 - val_loss: 2.5810 - val_acc: 0.4332\n",
            "Epoch 44/50\n",
            "2000/2000 [==============================] - 1s 667us/step - loss: 0.3285 - acc: 0.8815 - val_loss: 2.5197 - val_acc: 0.4395\n",
            "Epoch 45/50\n",
            "2000/2000 [==============================] - 1s 662us/step - loss: 0.2622 - acc: 0.9110 - val_loss: 2.6267 - val_acc: 0.4337\n",
            "Epoch 46/50\n",
            "2000/2000 [==============================] - 1s 678us/step - loss: 0.2298 - acc: 0.9220 - val_loss: 2.9950 - val_acc: 0.4340\n",
            "Epoch 47/50\n",
            "2000/2000 [==============================] - 1s 679us/step - loss: 0.2113 - acc: 0.9325 - val_loss: 2.6395 - val_acc: 0.4240\n",
            "Epoch 48/50\n",
            "2000/2000 [==============================] - 1s 672us/step - loss: 0.1939 - acc: 0.9315 - val_loss: 2.8310 - val_acc: 0.4113\n",
            "Epoch 49/50\n",
            "2000/2000 [==============================] - 1s 663us/step - loss: 0.1911 - acc: 0.9390 - val_loss: 2.8214 - val_acc: 0.4317\n",
            "Epoch 50/50\n",
            "2000/2000 [==============================] - 1s 685us/step - loss: 0.2076 - acc: 0.9295 - val_loss: 2.7772 - val_acc: 0.4377\n",
            "Accuracy: 43.77%\n",
            "loss: 277.72%\n",
            "Accuracy: 43.77%\n",
            "loss: 277.72%\n",
            "   ImageId  Label\n",
            "0        1      5\n",
            "1        2      1\n",
            "2        3      8\n",
            "3        4      0\n",
            "Actual labels for first four images: [3 8 8 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}